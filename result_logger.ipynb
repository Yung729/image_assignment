{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73397af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this to a new cell in your notebook - AFTER the ResultLogger class\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "class ResultsAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes and visualizes results from multiple member verification systems.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, results_directory: str = \"results\"):\n",
    "        \"\"\"\n",
    "        Initialize the analyzer.\n",
    "        \n",
    "        Args:\n",
    "            results_directory (str): Directory containing member result files\n",
    "        \"\"\"\n",
    "        self.results_dir = Path(results_directory)\n",
    "        \n",
    "    def load_all_results(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load all member results into a pandas DataFrame.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Combined results from all members\n",
    "        \"\"\"\n",
    "        all_results = []\n",
    "        \n",
    "        # Look for all *_results.json files\n",
    "        for filepath in self.results_dir.glob(\"*_results.json\"):\n",
    "            try:\n",
    "                with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                    all_results.append(data)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load {filepath}: {e}\")\n",
    "                \n",
    "        if not all_results:\n",
    "            print(\"No result files found!\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(all_results)\n",
    "        \n",
    "        # Convert timestamp to datetime if it exists\n",
    "        if 'timestamp' in df.columns:\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    def generate_comparison_table(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate a formatted comparison table of all members.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Formatted comparison table\n",
    "        \"\"\"\n",
    "        df = self.load_all_results()\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"No data to analyze!\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Create summary table\n",
    "        summary = df.copy()\n",
    "        \n",
    "        # Format score as percentage\n",
    "        summary['Score (%)'] = (summary['score'] * 100).round(2)\n",
    "        \n",
    "        # Format runtime \n",
    "        summary['Runtime (s)'] = summary['runtime'].round(4)\n",
    "        \n",
    "        # Reorder columns for display\n",
    "        display_cols = ['member', 'student_id', 'Score (%)', 'Runtime (s)', 'timestamp']\n",
    "        summary = summary[display_cols]\n",
    "        \n",
    "        # Rename columns for better display\n",
    "        summary.columns = ['Member', 'Student ID', 'Score (%)', 'Runtime (s)', 'Timestamp']\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def generate_comparison_graphs(self, save_plots: bool = True):\n",
    "        \"\"\"\n",
    "        Generate comparison graphs for member performance.\n",
    "        \n",
    "        Args:\n",
    "            save_plots (bool): Whether to save plots to files\n",
    "        \"\"\"\n",
    "        df = self.load_all_results()\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"No data to analyze!\")\n",
    "            return\n",
    "        \n",
    "        # Set up the plotting style\n",
    "        plt.style.use('default')\n",
    "        sns.set_palette(\"husl\")\n",
    "        \n",
    "        # Create a figure with multiple subplots\n",
    "        fig = plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # 1. Score Comparison Bar Chart\n",
    "        plt.subplot(2, 3, 1)\n",
    "        scores_pct = df['score'] * 100\n",
    "        bars = plt.bar(df['member'], scores_pct, alpha=0.8)\n",
    "        plt.title('Verification Scores by Member', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Member')\n",
    "        plt.ylabel('Score (%)')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, score in zip(bars, scores_pct):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                    f'{score:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # 2. Runtime Comparison Bar Chart\n",
    "        plt.subplot(2, 3, 2)\n",
    "        bars = plt.bar(df['member'], df['runtime'], alpha=0.8, color='orange')\n",
    "        plt.title('Runtime Comparison by Member', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Member')\n",
    "        plt.ylabel('Runtime (seconds)')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, runtime in zip(bars, df['runtime']):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                    f'{runtime:.3f}s', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # 3. Score vs Runtime Scatter Plot\n",
    "        plt.subplot(2, 3, 3)\n",
    "        scatter = plt.scatter(df['runtime'], df['score'] * 100, \n",
    "                            c=range(len(df)), cmap='viridis', s=100, alpha=0.8)\n",
    "        plt.title('Score vs Runtime', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Runtime (seconds)')\n",
    "        plt.ylabel('Score (%)')\n",
    "        \n",
    "        # Add member labels to points\n",
    "        for i, member in enumerate(df['member']):\n",
    "            plt.annotate(member, (df.iloc[i]['runtime'], df.iloc[i]['score'] * 100),\n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "        \n",
    "        plt.grid(alpha=0.3)\n",
    "        \n",
    "        # 4. Performance Ranking (Score/Runtime ratio)\n",
    "        plt.subplot(2, 3, 4)\n",
    "        # Higher score and lower runtime is better\n",
    "        performance_ratio = (df['score'] * 100) / (df['runtime'] + 0.001)  # Avoid division by zero\n",
    "        sorted_data = df.copy()\n",
    "        sorted_data['performance'] = performance_ratio\n",
    "        sorted_data = sorted_data.sort_values('performance', ascending=True)\n",
    "        \n",
    "        bars = plt.barh(sorted_data['member'], sorted_data['performance'], alpha=0.8, color='green')\n",
    "        plt.title('Performance Efficiency\\n(Score/Runtime Ratio)', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Efficiency Score')\n",
    "        plt.ylabel('Member')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, perf in zip(bars, sorted_data['performance']):\n",
    "            plt.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2, \n",
    "                    f'{perf:.1f}', ha='left', va='center', fontweight='bold')\n",
    "        \n",
    "        plt.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # 5. Detailed Comparison Table (as text)\n",
    "        plt.subplot(2, 3, 5)\n",
    "        plt.axis('off')  # Turn off axis\n",
    "        \n",
    "        # Create table data\n",
    "        table_data = []\n",
    "        for _, row in df.iterrows():\n",
    "            table_data.append([\n",
    "                row['member'],\n",
    "                f\"{row['score']*100:.1f}%\",\n",
    "                f\"{row['runtime']:.3f}s\",\n",
    "                row['student_id']\n",
    "            ])\n",
    "        \n",
    "        # Create table\n",
    "        table = plt.table(cellText=table_data,\n",
    "                         colLabels=['Member', 'Score', 'Runtime', 'Student ID'],\n",
    "                         cellLoc='center',\n",
    "                         loc='center',\n",
    "                         colWidths=[0.2, 0.2, 0.25, 0.25])\n",
    "        \n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(10)\n",
    "        table.scale(1, 2)\n",
    "        \n",
    "        # Style the table\n",
    "        table[(0, 0)].set_facecolor('#4CAF50')\n",
    "        table[(0, 1)].set_facecolor('#4CAF50')\n",
    "        table[(0, 2)].set_facecolor('#4CAF50')\n",
    "        table[(0, 3)].set_facecolor('#4CAF50')\n",
    "        \n",
    "        plt.title('Results Summary Table', fontsize=14, fontweight='bold', pad=20)\n",
    "        \n",
    "        # 6. Member Statistics Summary\n",
    "        plt.subplot(2, 3, 6)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Calculate statistics\n",
    "        stats_text = f\"\"\"\n",
    "        COMPARISON STATISTICS\n",
    "        \n",
    "        Highest Score: {df.loc[df['score'].idxmax(), 'member']} \n",
    "        ({df['score'].max()*100:.1f}%)\n",
    "        \n",
    "        Fastest Runtime: {df.loc[df['runtime'].idxmin(), 'member']} \n",
    "        ({df['runtime'].min():.3f}s)\n",
    "        \n",
    "        Average Score: {df['score'].mean()*100:.1f}%\n",
    "        Average Runtime: {df['runtime'].mean():.3f}s\n",
    "        \n",
    "        Score Range: {(df['score'].max() - df['score'].min())*100:.1f}%\n",
    "        Runtime Range: {df['runtime'].max() - df['runtime'].min():.3f}s\n",
    "        \"\"\"\n",
    "        \n",
    "        plt.text(0.1, 0.9, stats_text, fontsize=11, verticalalignment='top',\n",
    "                bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightblue\", alpha=0.8))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_plots:\n",
    "            # Save the plot\n",
    "            plt.savefig('member_comparison_analysis.png', dpi=300, bbox_inches='tight')\n",
    "            print(\"Graph saved as 'member_comparison_analysis.png'\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def print_detailed_analysis(self):\n",
    "        \"\"\"\n",
    "        Print detailed analysis of member performance.\n",
    "        \"\"\"\n",
    "        df = self.load_all_results()\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"No data to analyze!\")\n",
    "            return\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"DETAILED MEMBER PERFORMANCE ANALYSIS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            print(f\"\\n{row['member'].upper()}:\")\n",
    "            print(f\"  Student ID: {row['student_id']}\")\n",
    "            print(f\"  Score: {row['score']*100:.2f}%\")\n",
    "            print(f\"  Runtime: {row['runtime']:.4f} seconds\")\n",
    "            print(f\"  Timestamp: {row['timestamp']}\")\n",
    "            \n",
    "            # Performance assessment\n",
    "            if row['score'] >= 0.9:\n",
    "                score_assessment = \"EXCELLENT\"\n",
    "            elif row['score'] >= 0.8:\n",
    "                score_assessment = \"GOOD\"\n",
    "            elif row['score'] >= 0.7:\n",
    "                score_assessment = \"ACCEPTABLE\"\n",
    "            else:\n",
    "                score_assessment = \"NEEDS IMPROVEMENT\"\n",
    "                \n",
    "            if row['runtime'] <= 1.0:\n",
    "                speed_assessment = \"VERY FAST\"\n",
    "            elif row['runtime'] <= 2.0:\n",
    "                speed_assessment = \"FAST\"\n",
    "            elif row['runtime'] <= 3.0:\n",
    "                speed_assessment = \"MODERATE\"\n",
    "            else:\n",
    "                speed_assessment = \"SLOW\"\n",
    "                \n",
    "            print(f\"  Score Assessment: {score_assessment}\")\n",
    "            print(f\"  Speed Assessment: {speed_assessment}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"RANKING SUMMARY:\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Rank by score\n",
    "        score_ranking = df.sort_values('score', ascending=False)\n",
    "        print(\"\\nBY ACCURACY (Score):\")\n",
    "        for i, (_, row) in enumerate(score_ranking.iterrows(), 1):\n",
    "            print(f\"  {i}. {row['member']}: {row['score']*100:.1f}%\")\n",
    "        \n",
    "        # Rank by speed (lower is better)\n",
    "        speed_ranking = df.sort_values('runtime', ascending=True)\n",
    "        print(\"\\nBY SPEED (Runtime):\")\n",
    "        for i, (_, row) in enumerate(speed_ranking.iterrows(), 1):\n",
    "            print(f\"  {i}. {row['member']}: {row['runtime']:.3f}s\")\n",
    "        \n",
    "        # Overall performance (balance of score and speed)\n",
    "        df_copy = df.copy()\n",
    "        df_copy['overall_score'] = (df_copy['score'] * 100) / (df_copy['runtime'] + 0.1)\n",
    "        overall_ranking = df_copy.sort_values('overall_score', ascending=False)\n",
    "        print(\"\\nOVERALL PERFORMANCE (Score/Time Balance):\")\n",
    "        for i, (_, row) in enumerate(overall_ranking.iterrows(), 1):\n",
    "            print(f\"  {i}. {row['member']}: {row['overall_score']:.1f} efficiency score\")\n",
    "\n",
    "# Initialize the analyzer\n",
    "results_analyzer = ResultsAnalyzer()\n",
    "results_analyzer.generate_comparison_table()\n",
    "results_analyzer.generate_comparison_graphs()\n",
    "results_analyzer.print_detailed_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
